"""
æ ¹æ®åˆ›ä½œ idea ç”Ÿæˆå‰5ç« å†…å®¹

ä½¿ç”¨åˆ›ä½œåŠ©æ‰‹çš„å¤šå±‚çº§ç”Ÿæˆæµç¨‹ï¼š
ç®€ä»‹ -> å¤§çº² -> æ‘˜è¦ -> ç« èŠ‚
"""

import sys
import json
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from unimem.adapters.novel_adapter import NovelAdapter
from unimem.memory_types import Memory, Entity
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_creative_idea(idea_file: Path):
    """åŠ è½½åˆ›ä½œ idea"""
    with open(idea_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def generate_outline_from_synopsis(idea: dict, adapter: NovelAdapter):
    """ä»ç®€ä»‹ç”Ÿæˆå¤§çº²"""
    logger.info("="*50)
    logger.info("æ­¥éª¤1: ä»ç®€ä»‹ç”Ÿæˆæ•…äº‹å¤§çº²")
    logger.info("="*50)
    
    synopsis = idea.get('opening_scene', '') + "\n\n" + idea.get('background', '')
    
    outline = adapter.generate_from_hierarchy(
        synopsis=synopsis,
        target_level="outline",
        context_memories=[]
    )
    
    logger.info("âœ… å¤§çº²ç”Ÿæˆå®Œæˆ")
    return outline


def generate_summaries_from_outline(outline: str, adapter: NovelAdapter, num_chapters: int = 5):
    """ä»å¤§çº²ç”Ÿæˆç« èŠ‚æ‘˜è¦"""
    logger.info("\n" + "="*50)
    logger.info(f"æ­¥éª¤2: ä»å¤§çº²ç”Ÿæˆå‰{num_chapters}ç« æ‘˜è¦")
    logger.info("="*50)
    
    summaries = adapter.generate_from_hierarchy(
        synopsis=outline,
        target_level="summary",
        context_memories=[]
    )
    
    ***REMOVED*** è§£ææ‘˜è¦ï¼ˆå¯èƒ½æ˜¯ JSON æˆ–æ–‡æœ¬ï¼‰
    try:
        summaries_json = json.loads(summaries)
        if isinstance(summaries_json, dict) and 'summaries' in summaries_json:
            return summaries_json['summaries']
        elif isinstance(summaries_json, list):
            return summaries_json
    except:
        ***REMOVED*** å¦‚æœä¸æ˜¯ JSONï¼ŒæŒ‰è¡Œåˆ†å‰²
        if isinstance(summaries, str):
            summary_list = [s.strip() for s in summaries.split('\n') if s.strip()]
            return summary_list[:num_chapters]
    
    return [summaries] if summaries else []


def build_enhanced_prompt_with_feedback(original_prompt: str, validation_result: dict, attempt: int) -> str:
    """
    æ ¹æ®æ ¡éªŒåé¦ˆæ„å»ºå¢å¼ºçš„ prompt
    
    å°† issues å’Œ suggestions è½¬åŒ–ä¸ºå…·ä½“çš„ç”ŸæˆæŒ‡å¯¼
    
    Args:
        original_prompt: åŸå§‹ prompt
        validation_result: æ ¡éªŒç»“æœï¼ˆåŒ…å« issues, suggestions, coverageï¼‰
        attempt: å½“å‰å°è¯•æ¬¡æ•°
        
    Returns:
        å¢å¼ºåçš„ prompt
    """
    issues = validation_result.get('issues', [])
    suggestions = validation_result.get('suggestions', [])
    coverage = validation_result.get('coverage', {})
    
    feedback_section = "\n\nã€é‡è¦ï¼šæ ¹æ®ä¸Šä¸€ç‰ˆç”Ÿæˆç»“æœçš„åé¦ˆï¼Œè¯·ç‰¹åˆ«æ³¨æ„ä»¥ä¸‹è¦æ±‚ã€‘\n"
    
    ***REMOVED*** 1. æ˜ç¡®æŒ‡å‡ºéœ€è¦é¿å…çš„é—®é¢˜
    if issues:
        feedback_section += "\nã€éœ€è¦é¿å…çš„é—®é¢˜ã€‘\n"
        for i, issue in enumerate(issues[:5], 1):  ***REMOVED*** æœ€å¤š5ä¸ªé—®é¢˜
            feedback_section += f"{i}. {issue}\n"
        feedback_section += "\nè¯·ç¡®ä¿ç”Ÿæˆçš„å†…å®¹é¿å…ä¸Šè¿°é—®é¢˜ã€‚\n"
    
    ***REMOVED*** 2. æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®
    if suggestions:
        feedback_section += "\nã€æ”¹è¿›æ–¹å‘ã€‘\n"
        for i, suggestion in enumerate(suggestions[:5], 1):  ***REMOVED*** æœ€å¤š5æ¡å»ºè®®
            feedback_section += f"{i}. {suggestion}\n"
        feedback_section += "\nè¯·åœ¨ç”Ÿæˆæ—¶å……åˆ†è€ƒè™‘å¹¶ä½“ç°ä¸Šè¿°æ”¹è¿›å»ºè®®ã€‚\n"
    
    ***REMOVED*** 3. æ ¹æ®å„ç»´åº¦è¯„åˆ†æä¾›é’ˆå¯¹æ€§æŒ‡å¯¼
    if coverage:
        feedback_section += "\nã€å„ç»´åº¦è¦æ±‚ã€‘\n"
        if coverage.get('plot_coverage', 1.0) < 0.8:
            feedback_section += "- è¯·ç¡®ä¿å®Œæ•´æ¶µç›–é¢„æœŸæ‘˜è¦çš„æ ¸å¿ƒæƒ…èŠ‚ï¼Œä¸è¦é—æ¼é‡è¦æƒ…èŠ‚ç‚¹\n"
        if coverage.get('character_consistency', 1.0) < 0.8:
            feedback_section += "- è¯·ä¸¥æ ¼éµå¾ªåˆ›ä½œè®¾å®šä¸­çš„äººç‰©æ€§æ ¼å’Œè¡Œä¸ºç‰¹å¾\n"
        if coverage.get('setting_consistency', 1.0) < 0.8:
            feedback_section += "- è¯·ç¡®ä¿ä¸–ç•Œè§‚ã€èƒŒæ™¯è®¾å®šä¸åˆ›ä½œè®¾å®šå®Œå…¨ä¸€è‡´\n"
        if coverage.get('style_consistency', 1.0) < 0.8:
            feedback_section += "- è¯·ä¿æŒä¸åˆ›ä½œè®¾å®šè¦æ±‚çš„å†™ä½œé£æ ¼ä¸€è‡´\n"
    
    feedback_section += f"\nè¿™æ˜¯ç¬¬ {attempt} æ¬¡ç”Ÿæˆå°è¯•ï¼Œè¯·åŠ¡å¿…è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œç”Ÿæˆæ›´ç¬¦åˆè¦æ±‚çš„å†…å®¹ã€‚\n"
    
    return original_prompt + feedback_section


def validate_generated_chapter(chapter_content: str, expected_summary: str, idea: dict, adapter: NovelAdapter) -> dict:
    """
    åå‘ç»“æ„åŒ–æ ¡éªŒï¼šå¯¹ç”Ÿæˆçš„ç« èŠ‚è¿›è¡Œç»“æ„åŒ–åˆ†æï¼Œå¹¶ä¸é¢„æœŸæ‘˜è¦å¯¹æ¯”
    
    Returns:
        {
            'accuracy_score': float,  ***REMOVED*** å‡†ç¡®åº¦è¯„åˆ† 0-1
            'validation_result': dict,  ***REMOVED*** è¯¦ç»†æ ¡éªŒç»“æœ
            'issues': list,  ***REMOVED*** å‘ç°çš„é—®é¢˜
            'suggestions': list  ***REMOVED*** ä¼˜åŒ–å»ºè®®
        }
    """
    if not adapter.is_available():
        return {'accuracy_score': 0.5, 'validation_result': {}, 'issues': [], 'suggestions': []}
    
    try:
        ***REMOVED*** 1. å¯¹ç”Ÿæˆå†…å®¹è¿›è¡Œåå‘ç»“æ„åŒ–åˆ†æ
        logger.info("  è¿›è¡Œåå‘ç»“æ„åŒ–åˆ†æ...")
        generated_analysis = adapter._analyze_content(
            chapter_content[:2000],  ***REMOVED*** é™åˆ¶é•¿åº¦
            is_creative_content=True
        )
        
        ***REMOVED*** 2. å¯¹é¢„æœŸæ‘˜è¦è¿›è¡Œç»“æ„åŒ–åˆ†æ
        expected_analysis = adapter._analyze_content(
            expected_summary,
            is_creative_content=True
        )
        
        ***REMOVED*** 3. å¯¹æ¯”åˆ†æ
        logger.info("  è¿›è¡Œå¯¹æ¯”æ ¡éªŒ...")
        from unimem.chat import ark_deepseek_v3_2
        
        comparison_prompt = f"""è¯·å¯¹æ¯”ä»¥ä¸‹ä¸¤æ®µå†…å®¹çš„ç»“æ„åŒ–åˆ†æç»“æœï¼Œè¯„ä¼°ç”Ÿæˆå†…å®¹æ˜¯å¦ç¬¦åˆé¢„æœŸè¦æ±‚ã€‚

ã€é¢„æœŸæ‘˜è¦çš„ç»“æ„åŒ–åˆ†æã€‘
å…³é”®è¯: {', '.join(expected_analysis.get('keywords', [])[:10])}
ä¸Šä¸‹æ–‡: {expected_analysis.get('context', '')}
åˆ›ä½œç»´åº¦: {json.dumps(expected_analysis.get('creative_dimensions', {}), ensure_ascii=False)}

ã€ç”Ÿæˆç« èŠ‚çš„ç»“æ„åŒ–åˆ†æã€‘
å…³é”®è¯: {', '.join(generated_analysis.get('keywords', [])[:10])}
ä¸Šä¸‹æ–‡: {generated_analysis.get('context', '')}
åˆ›ä½œç»´åº¦: {json.dumps(generated_analysis.get('creative_dimensions', {}), ensure_ascii=False)}

ã€åˆ›ä½œè®¾å®šè¦æ±‚ã€‘
- ä½œå“ç±»å‹: {idea.get('genre', '')}
- ä¸»è¦äººç‰©: {json.dumps([c.get('name') for c in idea.get('main_characters', [])[:3]], ensure_ascii=False)}
- æ ¸å¿ƒå†²çª: {idea.get('core_conflict', '')[:200]}

è¯·è¯„ä¼°ï¼š
1. ç”Ÿæˆå†…å®¹æ˜¯å¦æ¶µç›–äº†é¢„æœŸæ‘˜è¦çš„æ ¸å¿ƒæƒ…èŠ‚ï¼Ÿ
2. ç”Ÿæˆå†…å®¹æ˜¯å¦ç¬¦åˆåˆ›ä½œè®¾å®šï¼ˆäººç‰©ã€ä¸–ç•Œè§‚ã€é£æ ¼ï¼‰ï¼Ÿ
3. æ˜¯å¦å­˜åœ¨æ˜æ˜¾åå·®æˆ–é—æ¼ï¼Ÿ

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š
{{
    "accuracy_score": 0.0-1.0,  // å‡†ç¡®åº¦è¯„åˆ†
    "coverage": {{
        "plot_coverage": 0.0-1.0,  // æƒ…èŠ‚è¦†ç›–åº¦
        "character_consistency": 0.0-1.0,  // äººç‰©ä¸€è‡´æ€§
        "setting_consistency": 0.0-1.0,  // è®¾å®šä¸€è‡´æ€§
        "style_consistency": 0.0-1.0  // é£æ ¼ä¸€è‡´æ€§
    }},
    "issues": [
        "é—®é¢˜1æè¿°",
        "é—®é¢˜2æè¿°"
    ],
    "suggestions": [
        "ä¼˜åŒ–å»ºè®®1",
        "ä¼˜åŒ–å»ºè®®2"
    ]
}}"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹è´¨é‡è¯„ä¼°åŠ©æ‰‹ï¼Œæ“…é•¿å¯¹æ¯”åˆ†ææ–‡æœ¬å†…å®¹æ˜¯å¦ç¬¦åˆé¢„æœŸè¦æ±‚ã€‚è¯·å§‹ç»ˆä»¥æœ‰æ•ˆçš„ JSON æ ¼å¼è¿”å›ç»“æœã€‚"},
            {"role": "user", "content": comparison_prompt}
        ]
        
        _, response_text = ark_deepseek_v3_2(messages, max_new_tokens=1024)
        validation_result = adapter._parse_json_response(response_text)
        
        if validation_result:
            accuracy_score = validation_result.get('accuracy_score', 0.5)
            logger.info(f"  âœ… æ ¡éªŒå®Œæˆï¼Œå‡†ç¡®åº¦è¯„åˆ†: {accuracy_score:.2f}")
            return {
                'accuracy_score': accuracy_score,
                'validation_result': validation_result,
                'issues': validation_result.get('issues', []),
                'suggestions': validation_result.get('suggestions', []),
                'coverage': validation_result.get('coverage', {})
            }
        else:
            logger.warning("  æ ¡éªŒç»“æœè§£æå¤±è´¥")
            return {'accuracy_score': 0.5, 'validation_result': {}, 'issues': [], 'suggestions': []}
            
    except Exception as e:
        logger.error(f"  æ ¡éªŒè¿‡ç¨‹å‡ºé”™: {e}")
        return {'accuracy_score': 0.5, 'validation_result': {}, 'issues': [], 'suggestions': []}


def generate_chapter_from_summary(summary: str, idea: dict, chapter_num: int, adapter: NovelAdapter, context_memories: list, enable_validation: bool = True, validation_threshold: float = 0.7, max_retries: int = 2):
    """
    ä»æ‘˜è¦ç”Ÿæˆå®Œæ•´ç« èŠ‚ï¼ˆå¸¦åå‘ç»“æ„åŒ–æ ¡éªŒï¼‰
    
    Args:
        summary: ç« èŠ‚æ‘˜è¦
        idea: åˆ›ä½œè®¾å®š
        chapter_num: ç« èŠ‚ç¼–å·
        adapter: é€‚é…å™¨
        context_memories: ä¸Šä¸‹æ–‡è®°å¿†
        enable_validation: æ˜¯å¦å¯ç”¨æ ¡éªŒ
        validation_threshold: æ ¡éªŒé˜ˆå€¼ï¼ˆä½äºæ­¤å€¼ä¼šé‡æ–°ç”Ÿæˆï¼‰
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    """
    logger.info(f"\nç”Ÿæˆç¬¬ {chapter_num} ç« ...")
    
    ***REMOVED*** å¦‚æœæœ‰å‰ç« èŠ‚çš„æ ¡éªŒåé¦ˆï¼ŒåŠ å…¥æŒ‡å¯¼
    previous_feedback = ""
    if 'validation_feedback' in idea and idea['validation_feedback']:
        ***REMOVED*** è·å–å‰ç« èŠ‚çš„åé¦ˆï¼ˆç‰¹åˆ«æ˜¯æœ€è¿‘ä¸€ç« ï¼‰
        recent_feedback = idea['validation_feedback'][-1]
        if recent_feedback.get('chapter') < chapter_num:
            previous_feedback = "\n\nã€å‚è€ƒï¼šå‰ç« èŠ‚çš„æ”¹è¿›å»ºè®®ã€‘\n"
            if recent_feedback.get('suggestions'):
                previous_feedback += "ä»¥ä¸‹å»ºè®®æ¥è‡ªå‰ç« èŠ‚çš„æ ¡éªŒåé¦ˆï¼Œè¯·åœ¨ç”Ÿæˆæ—¶å‚è€ƒï¼š\n"
                for i, sug in enumerate(recent_feedback['suggestions'], 1):
                    previous_feedback += f"{i}. {sug}\n"
    
    ***REMOVED*** æ„å»ºç« èŠ‚ç”Ÿæˆ promptï¼ˆåŒ…å«åˆ›ä½œ idea çš„ä¸Šä¸‹æ–‡ï¼‰
    chapter_prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹æ‘˜è¦å’Œåˆ›ä½œè®¾å®šï¼Œç”Ÿæˆå®Œæ•´çš„ç¬¬{chapter_num}ç« å†…å®¹ã€‚

åˆ›ä½œè®¾å®šï¼š
- ä½œå“ç±»å‹: {idea.get('genre', '')}
- å†™ä½œé£æ ¼: {idea.get('writing_style', '')}
- ä¸»è¦äººç‰©: {json.dumps(idea.get('main_characters', [])[:3], ensure_ascii=False)}
- æ ¸å¿ƒå†²çª: {idea.get('core_conflict', '')}
- ç‹¬ç‰¹è®¾å®š: {idea.get('unique_setting', '')}

ç« èŠ‚æ‘˜è¦ï¼š
{summary}
{previous_feedback}
è¦æ±‚ï¼š
1. ç« èŠ‚é•¿åº¦æ§åˆ¶åœ¨ 2000-3000 å­—å·¦å³
2. ä¿æŒä¸åˆ›ä½œè®¾å®šçš„é£æ ¼ä¸€è‡´
3. åŒ…å«åœºæ™¯æå†™ã€äººç‰©å¯¹è¯ã€æƒ…èŠ‚æ¨è¿›
4. ä¸ºä¸‹ä¸€ç« åŸ‹ä¸‹ä¼ç¬”
5. ä½¿ç”¨å¤å…¸ç™½è¯æ–‡é£æ ¼ï¼Œç¬¦åˆå†å²æ­¦ä¾ å°è¯´çš„è¯­è¨€ç‰¹è‰²

è¯·ç›´æ¥è¿”å›ç« èŠ‚å†…å®¹ï¼Œä¸è¦åŒ…å«æ ‡é¢˜æˆ–å…¶ä»–æ ¼å¼ï¼š"""
    
    for attempt in range(max_retries + 1):
        try:
            from unimem.chat import ark_deepseek_v3_2
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ­¦ä¾ å°è¯´åˆ›ä½œåŠ©æ‰‹ï¼Œæ“…é•¿æ ¹æ®è®¾å®šå’Œæ‘˜è¦ç”Ÿæˆå®Œæ•´çš„ç« èŠ‚å†…å®¹ã€‚è¯·ä¿æŒé£æ ¼ä¸€è‡´æ€§å’Œæƒ…èŠ‚è¿è´¯æ€§ã€‚"},
                {"role": "user", "content": chapter_prompt}
            ]
            
            _, response_text = ark_deepseek_v3_2(messages, max_new_tokens=4096)
            
            ***REMOVED*** æ¸…ç†å“åº”æ–‡æœ¬
            chapter_content = response_text.strip()
            if chapter_content.startswith("```"):
                ***REMOVED*** ç§»é™¤ markdown ä»£ç å—æ ‡è®°
                lines = chapter_content.split('\n')
                chapter_content = '\n'.join([line for line in lines if not line.strip().startswith('```')])
            
            ***REMOVED*** åå‘ç»“æ„åŒ–æ ¡éªŒ
            validation_result = None
            if enable_validation:
                validation_result = validate_generated_chapter(
                    chapter_content, summary, idea, adapter
                )
                
                accuracy_score = validation_result.get('accuracy_score', 0.5)
                
                if accuracy_score < validation_threshold and attempt < max_retries:
                    logger.warning(f"  âš ï¸ å‡†ç¡®åº¦è¯„åˆ† {accuracy_score:.2f} ä½äºé˜ˆå€¼ {validation_threshold}ï¼Œå°è¯•é‡æ–°ç”Ÿæˆ...")
                    
                    ***REMOVED*** ä½¿ç”¨ issues å’Œ suggestions æŒ‡å¯¼é‡æ–°ç”Ÿæˆ
                    enhanced_prompt = build_enhanced_prompt_with_feedback(
                        original_prompt=chapter_prompt,
                        validation_result=validation_result,
                        attempt=attempt + 1
                    )
                    chapter_prompt = enhanced_prompt
                    logger.info(f"  ğŸ“ å·²æ ¹æ®æ ¡éªŒåé¦ˆä¼˜åŒ– promptï¼ˆç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼‰")
                    continue
                else:
                    logger.info(f"  âœ… æ ¡éªŒé€šè¿‡ï¼Œå‡†ç¡®åº¦è¯„åˆ†: {accuracy_score:.2f}")
                    if validation_result.get('issues'):
                        logger.info(f"  âš ï¸ å‘ç° {len(validation_result['issues'])} ä¸ªé—®é¢˜ï¼ˆä½†ä¸å½±å“ä½¿ç”¨ï¼‰")
                        ***REMOVED*** å³ä½¿é€šè¿‡ï¼Œä¹Ÿè®°å½• issues å’Œ suggestions ä¾›åç»­ç« èŠ‚å‚è€ƒ
                        if validation_result.get('suggestions'):
                            logger.info(f"  ğŸ’¡ ç”Ÿæˆ {len(validation_result['suggestions'])} æ¡ä¼˜åŒ–å»ºè®®ï¼ˆå·²è®°å½•ï¼‰")
                            ***REMOVED*** å°†é‡è¦çš„é—®é¢˜å’Œå»ºè®®å­˜å‚¨åˆ° idea çš„ metadata ä¸­ï¼Œä¾›åç»­ç« èŠ‚å‚è€ƒ
                            if 'validation_feedback' not in idea:
                                idea['validation_feedback'] = []
                            idea['validation_feedback'].append({
                                'chapter': chapter_num,
                                'issues': validation_result.get('issues', [])[:3],  ***REMOVED*** åªä¿ç•™æœ€é‡è¦çš„3ä¸ª
                                'suggestions': validation_result.get('suggestions', [])[:3]  ***REMOVED*** åªä¿ç•™æœ€é‡è¦çš„3æ¡
                            })
            
            logger.info(f"âœ… ç¬¬ {chapter_num} ç« ç”Ÿæˆå®Œæˆ ({len(chapter_content)} å­—)")
            return chapter_content, validation_result
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç¬¬ {chapter_num} ç« å¤±è´¥: {e}")
            if attempt < max_retries:
                logger.info(f"  é‡è¯•ä¸­... ({attempt + 1}/{max_retries})")
                continue
            return "", None
    
    return "", None


def generate_chapters(idea: dict, adapter: NovelAdapter, num_chapters: int = 5, enable_validation: bool = True, validation_threshold: float = 0.7, validate_first_n: int = 3):
    """
    ç”Ÿæˆå‰Nç« å†…å®¹ï¼ˆæ”¯æŒåå‘ç»“æ„åŒ–æ ¡éªŒï¼‰
    
    Args:
        idea: åˆ›ä½œè®¾å®š
        adapter: é€‚é…å™¨
        num_chapters: ç« èŠ‚æ•°é‡
        enable_validation: æ˜¯å¦å¯ç”¨æ ¡éªŒ
        validation_threshold: æ ¡éªŒé˜ˆå€¼ï¼ˆ0-1ï¼‰
        validate_first_n: å‰Nç« å¿…é¡»æ ¡éªŒï¼ˆå…¶ä½™ç« èŠ‚å¯é€‰ï¼‰
    """
    logger.info("="*60)
    logger.info(f"å¼€å§‹ç”Ÿæˆã€Š{idea.get('title', '')}ã€‹å‰ {num_chapters} ç« ")
    if enable_validation:
        logger.info(f"æ ¡éªŒæ¨¡å¼: å‰ {validate_first_n} ç« å¼ºåˆ¶æ ¡éªŒï¼Œé˜ˆå€¼ {validation_threshold}")
    logger.info("="*60)
    
    ***REMOVED*** 1. ç”Ÿæˆå¤§çº²
    outline = generate_outline_from_synopsis(idea, adapter)
    
    ***REMOVED*** ä¿å­˜å¤§çº²
    outline_file = Path(__file__).parent.parent.parent / 'data' / 'xuemolu_outline.json'
    with open(outline_file, 'w', encoding='utf-8') as f:
        json.dump({"outline": outline}, f, ensure_ascii=False, indent=2)
    logger.info(f"å¤§çº²å·²ä¿å­˜åˆ°: {outline_file}")
    
    ***REMOVED*** 2. ç”Ÿæˆç« èŠ‚æ‘˜è¦
    summaries = generate_summaries_from_outline(outline, adapter, num_chapters)
    
    if not summaries:
        logger.warning("æœªèƒ½ç”Ÿæˆç« èŠ‚æ‘˜è¦ï¼Œä½¿ç”¨é»˜è®¤æ‘˜è¦")
        summaries = [f"ç¬¬{i}ç« æ‘˜è¦" for i in range(1, num_chapters + 1)]
    
    ***REMOVED*** ä¿å­˜æ‘˜è¦
    summaries_file = Path(__file__).parent.parent.parent / 'data' / 'xuemolu_summaries.json'
    with open(summaries_file, 'w', encoding='utf-8') as f:
        json.dump({"summaries": summaries}, f, ensure_ascii=False, indent=2)
    logger.info(f"ç« èŠ‚æ‘˜è¦å·²ä¿å­˜åˆ°: {summaries_file}")
    
    ***REMOVED*** 3. ç”Ÿæˆå„ç« èŠ‚å†…å®¹ï¼ˆé€ç« ç”Ÿæˆï¼Œæ¯ç« å•ç‹¬ä¿å­˜ï¼‰
    data_dir = Path(__file__).parent.parent.parent / 'data'
    
    ***REMOVED*** åˆ›å»ºä¿å­˜ç›®å½•
    novel_dir = data_dir / 'novel'
    novel_dir.mkdir(exist_ok=True)
    logger.info(f"ç« èŠ‚ä¿å­˜ç›®å½•: {novel_dir}")
    
    chapters = []
    context_memories = []  ***REMOVED*** ç”¨äºç´¯ç§¯ä¸Šä¸‹æ–‡
    
    for i, summary in enumerate(summaries[:num_chapters], 1):
        ***REMOVED*** å†³å®šæ˜¯å¦å¯ç”¨æ ¡éªŒï¼šå‰Nç« å¼ºåˆ¶æ ¡éªŒï¼Œå…¶ä½™ç« èŠ‚å¯é€‰
        should_validate = enable_validation and (i <= validate_first_n)
        
        chapter_content, validation_result = generate_chapter_from_summary(
            summary=summary,
            idea=idea,
            chapter_num=i,
            adapter=adapter,
            context_memories=context_memories,
            enable_validation=should_validate,
            validation_threshold=validation_threshold
        )
        
        if chapter_content:
            chapter_data = {
                'chapter_num': i,
                'title': f"ç¬¬{i}ç« ",
                'summary': summary,
                'content': chapter_content,
                'word_count': len(chapter_content)
            }
            
            ***REMOVED*** æ·»åŠ æ ¡éªŒç»“æœ
            if validation_result:
                chapter_data['validation'] = {
                    'accuracy_score': validation_result.get('accuracy_score', 0),
                    'coverage': validation_result.get('coverage', {}),
                    'issues': validation_result.get('issues', []),
                    'suggestions': validation_result.get('suggestions', [])
                }
            
            chapters.append(chapter_data)
            
            ***REMOVED*** ç«‹å³ä¿å­˜å•ç« åˆ°æ–‡ä»¶
            chapter_file = novel_dir / f"chapter_{i:02d}.json"
            with open(chapter_file, 'w', encoding='utf-8') as f:
                json.dump(chapter_data, f, ensure_ascii=False, indent=2)
            
            ***REMOVED*** åŒæ—¶ä¿å­˜çº¯æ–‡æœ¬ç‰ˆæœ¬
            chapter_txt_file = novel_dir / f"chapter_{i:02d}.txt"
            with open(chapter_txt_file, 'w', encoding='utf-8') as f:
                f.write(f"ç¬¬{i}ç« ï¼š{chapter_data['title']}\n")
                f.write("="*60 + "\n\n")
                f.write(chapter_content)
            
            logger.info(f"  âœ… ç¬¬ {i} ç« å®Œæˆ ({len(chapter_content)} å­—) - å·²ä¿å­˜åˆ° {chapter_file.name}")
            
            ***REMOVED*** å°†ç”Ÿæˆçš„ç« èŠ‚å†…å®¹å­˜å‚¨ä¸ºè®°å¿†ï¼Œç”¨äºåç»­ç« èŠ‚çš„ä¸Šä¸‹æ–‡
            if adapter.is_available():
                memory = adapter.construct_atomic_note(
                    content=chapter_content[:1000],  ***REMOVED*** åªå­˜å‚¨éƒ¨åˆ†å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
                    timestamp=datetime.now(),
                    entities=[],
                    generate_summary=False,
                    is_creative_content=True
                )
                context_memories.append(memory)
                if len(context_memories) > 3:  ***REMOVED*** åªä¿ç•™æœ€è¿‘3ç« ä½œä¸ºä¸Šä¸‹æ–‡
                    context_memories = context_memories[-3:]
        
        ***REMOVED*** æ·»åŠ å»¶è¿Ÿé¿å… API é™æµ
        import time
        time.sleep(1)
    
    return chapters


def save_chapters(chapters: list, idea: dict, output_file: Path):
    """
    ä¿å­˜ç« èŠ‚æ±‡æ€»åˆ°æ–‡ä»¶
    
    æ³¨æ„ï¼šå„ç« èŠ‚å·²å•ç‹¬ä¿å­˜åˆ° data/novel/ ç›®å½•ï¼Œæ­¤å‡½æ•°ç”¨äºä¿å­˜æ±‡æ€»æ–‡ä»¶ã€‚
    """
    output_data = {
        'novel_info': {
            'title': idea.get('title', ''),
            'genre': idea.get('genre', ''),
            'background': idea.get('background', ''),
            'core_conflict': idea.get('core_conflict', ''),
            'total_chapters': len(chapters),
            'generated_chapters': len(chapters)
        },
        'chapters': chapters,
        'generation_time': datetime.now().isoformat()
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nâœ… ç« èŠ‚æ±‡æ€»å·²ä¿å­˜åˆ°: {output_file}")
    
    ***REMOVED*** åŒæ—¶ç”Ÿæˆçº¯æ–‡æœ¬ç‰ˆæœ¬
    txt_file = output_file.with_suffix('.txt')
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write(f"{idea.get('title', '')}\n")
        f.write("="*60 + "\n\n")
        for ch in chapters:
            f.write(f"{ch['title']}\n")
            f.write("-"*60 + "\n\n")
            f.write(ch['content'])
            f.write("\n\n" + "="*60 + "\n\n")
    
    logger.info(f"âœ… çº¯æ–‡æœ¬æ±‡æ€»ç‰ˆæœ¬å·²ä¿å­˜åˆ°: {txt_file}")


def main():
    ***REMOVED*** åˆå§‹åŒ–é€‚é…å™¨
    config = {
        'local_model_path': '/root/data/AI/pretrain/multilingual-e5-small',
        'qdrant_host': 'localhost',
        'qdrant_port': 6333,
        'collection_name': 'xuemolu_creation'
    }
    
    adapter = NovelAdapter(config)
    adapter.initialize()
    
    ***REMOVED*** åŠ è½½åˆ›ä½œ idea
    idea_file = Path(__file__).parent.parent.parent / 'data' / 'new_idea.json'
    logger.info(f"åŠ è½½åˆ›ä½œ idea: {idea_file}")
    idea = load_creative_idea(idea_file)
    
    logger.info(f"ä½œå“æ ‡é¢˜: {idea.get('title', '')}")
    logger.info(f"ä½œå“ç±»å‹: {idea.get('genre', '')}")
    
    ***REMOVED*** ç”Ÿæˆå‰5ç« ï¼ˆå¯ç”¨æ ¡éªŒï¼Œå‰3ç« å¼ºåˆ¶æ ¡éªŒï¼Œé˜ˆå€¼0.7ï¼‰
    chapters = generate_chapters(
        idea, 
        adapter, 
        num_chapters=5,
        enable_validation=True,  ***REMOVED*** å¯ç”¨æ ¡éªŒ
        validation_threshold=0.7,  ***REMOVED*** å‡†ç¡®åº¦é˜ˆå€¼
        validate_first_n=3  ***REMOVED*** å‰3ç« å¼ºåˆ¶æ ¡éªŒ
    )
    
    ***REMOVED*** ä¿å­˜ç« èŠ‚æ±‡æ€»ï¼ˆæ‰€æœ‰ç« èŠ‚å·²å•ç‹¬ä¿å­˜ï¼Œè¿™é‡Œä¿å­˜æ±‡æ€»ï¼‰
    data_dir = Path(__file__).parent.parent.parent / 'data'
    output_file = data_dir / 'xuemolu_chapters.json'
    save_chapters(chapters, idea, output_file)
    
    ***REMOVED*** æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "="*60)
    logger.info("ç”Ÿæˆç»Ÿè®¡")
    logger.info("="*60)
    logger.info(f"æ€»ç« èŠ‚æ•°: {len(chapters)}")
    total_words = sum(ch.get('word_count', 0) for ch in chapters)
    logger.info(f"æ€»å­—æ•°: {total_words:,} å­—")
    logger.info(f"å¹³å‡æ¯ç« : {total_words // len(chapters) if chapters else 0} å­—")
    logger.info(f"å•ç« æ–‡ä»¶å·²ä¿å­˜åˆ°: {data_dir / 'novel'}/")
    
    ***REMOVED*** æ‰“å°æ ¡éªŒç»Ÿè®¡
    validated_chapters = [ch for ch in chapters if 'validation' in ch]
    if validated_chapters:
        logger.info(f"\næ ¡éªŒç»Ÿè®¡:")
        logger.info(f"  æ ¡éªŒç« èŠ‚æ•°: {len(validated_chapters)}")
        avg_accuracy = sum(ch['validation'].get('accuracy_score', 0) for ch in validated_chapters) / len(validated_chapters)
        logger.info(f"  å¹³å‡å‡†ç¡®åº¦: {avg_accuracy:.2f}")
        for ch in validated_chapters:
            score = ch['validation'].get('accuracy_score', 0)
            logger.info(f"    ç¬¬{ch['chapter_num']}ç« : {score:.2f}")
    
    logger.info("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼")


if __name__ == "__main__":
    main()

