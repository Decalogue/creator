***REMOVED***!/usr/bin/env python3
"""
LLMæ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬

å¯¹æ¯” deepseek_v3_2 å’Œ gemini_3_flash åœ¨å°è¯´åˆ›ä½œä¸­çš„è¡¨ç°ï¼š
1. ç« èŠ‚ç”Ÿæˆè´¨é‡
2. å­—æ•°æ§åˆ¶
3. é‡å†™æ•ˆæœ
4. è´¨é‡é—®é¢˜æ•°é‡
"""
import sys
import os
import argparse
import logging
import json
from pathlib import Path
from typing import Dict, Any, Optional
from datetime import datetime

***REMOVED*** æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from novel_creation.react_novel_creator import ReactNovelCreator

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def test_with_model(
    model_name: str,
    novel_title: str,
    genre: str,
    theme: str,
    target_chapters: int,
    words_per_chapter: int = 2048,
    test_title: str = None
) -> Dict[str, Any]:
    """
    ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿›è¡Œæµ‹è¯•
    
    Returns:
        æµ‹è¯•ç»“æœç»Ÿè®¡
    """
    logger.info("=" * 80)
    logger.info(f"å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_name}")
    logger.info("=" * 80)
    
    ***REMOVED*** é€‰æ‹©LLMå®¢æˆ·ç«¯
    if model_name == "gemini_3_flash":
        from llm.chat import gemini_3_flash
        llm_client = gemini_3_flash
    elif model_name == "deepseek_v3_2":
        from llm.chat import deepseek_v3_2
        llm_client = deepseek_v3_2
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
    
    logger.info(f"ä½¿ç”¨LLMæ¨¡å‹: {model_name}")
    
    ***REMOVED*** åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = f"novel_creation/outputs/{test_title}_{model_name}"
    
    ***REMOVED*** åˆ›å»ºåˆ›ä½œå™¨
    creator = ReactNovelCreator(
        novel_title=novel_title,
        output_dir=output_dir,
        enable_quality_check=True,
        enable_creative_context=True,
        llm_client=llm_client
    )
    
    try:
        ***REMOVED*** åˆ›ä½œå°è¯´
        result = creator.create_novel(
            genre=genre,
            theme=theme,
            target_chapters=target_chapters,
            words_per_chapter=words_per_chapter,
            use_progressive=target_chapters >= 50
        )
        
        ***REMOVED*** ç»Ÿè®¡ç»“æœ
        stats = analyze_results(creator, output_dir, model_name)
        return stats
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        return {
            "model": model_name,
            "error": str(e),
            "success": False
        }


def analyze_results(creator: ReactNovelCreator, output_dir: str, model_name: str) -> Dict[str, Any]:
    """
    åˆ†ææµ‹è¯•ç»“æœ
    """
    chapters_dir = Path(output_dir) / "chapters"
    
    stats = {
        "model": model_name,
        "success": True,
        "total_chapters": 0,
        "total_words": 0,
        "avg_words_per_chapter": 0,
        "word_control": {
            "avg_diff_percent": 0,
            "chapters_within_target": 0,
            "chapters_over_target": 0,
        },
        "quality": {
            "total_issues": 0,
            "avg_issues_per_chapter": 0,
            "chapters_with_issues": 0,
            "high_severity_issues": 0,
        },
        "rewrite": {
            "rewritten_chapters": 0,
            "improved_chapters": 0,
            "unchanged_chapters": 0,
            "worsened_chapters": 0,
            "avg_rewrite_rounds": 0,
            "improvement_rate": 0,
        },
        "issue_types": {},
    }
    
    ***REMOVED*** è¯»å–metadata
    metadata_file = Path(output_dir) / "metadata.json"
    if metadata_file.exists():
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        stats["total_chapters"] = metadata.get("total_chapters", 0)
        stats["total_words"] = metadata.get("total_words", 0)
        stats["avg_words_per_chapter"] = stats["total_words"] / max(stats["total_chapters"], 1)
    
    ***REMOVED*** åˆ†æå„ç« èŠ‚
    word_diffs = []
    total_rewrite_rounds = 0
    
    for i in range(1, stats["total_chapters"] + 1):
        meta_file = chapters_dir / f"chapter_{i:03d}_meta.json"
        if not meta_file.exists():
            continue
        
        with open(meta_file, 'r', encoding='utf-8') as f:
            chapter_meta = json.load(f)
        
        metadata = chapter_meta.get('metadata', {})
        
        ***REMOVED*** å­—æ•°ç»Ÿè®¡
        target_words = metadata.get('target_words', 2048)
        actual_words = metadata.get('actual_words', 0)
        word_diff_percent = metadata.get('word_diff_percent', 0)
        word_diffs.append(word_diff_percent)
        
        if abs(word_diff_percent) <= 10:
            stats["word_control"]["chapters_within_target"] += 1
        elif word_diff_percent > 10:
            stats["word_control"]["chapters_over_target"] += 1
        
        ***REMOVED*** è´¨é‡é—®é¢˜ç»Ÿè®¡
        quality_check = metadata.get('quality_check', {})
        if isinstance(quality_check, dict) and quality_check:
            issues_count = quality_check.get('total_issues', 0)
            if issues_count > 0:
                stats["quality"]["chapters_with_issues"] += 1
                stats["quality"]["total_issues"] += issues_count
                
                ***REMOVED*** ç»Ÿè®¡é—®é¢˜ç±»å‹
                issues = quality_check.get('issues', [])
                for issue in issues:
                    issue_type = issue.get('type', 'unknown')
                    stats["issue_types"][issue_type] = stats["issue_types"].get(issue_type, 0) + 1
                
                ***REMOVED*** ä¸¥é‡é—®é¢˜
                by_severity = quality_check.get('by_severity', {})
                if isinstance(by_severity, dict):
                    stats["quality"]["high_severity_issues"] += by_severity.get('high', 0)
        
        ***REMOVED*** é‡å†™ç»Ÿè®¡
        if metadata.get('rewritten', False):
            stats["rewrite"]["rewritten_chapters"] += 1
            rewrite_rounds = metadata.get('rewrite_rounds', 0)
            total_rewrite_rounds += rewrite_rounds
            
            original_issues = metadata.get('original_issue_count', 0)
            final_issues = metadata.get('final_issue_count', 0)
            
            if isinstance(original_issues, int) and isinstance(final_issues, int):
                if final_issues < original_issues:
                    stats["rewrite"]["improved_chapters"] += 1
                elif final_issues == original_issues:
                    stats["rewrite"]["unchanged_chapters"] += 1
                elif final_issues > original_issues:
                    stats["rewrite"]["worsened_chapters"] += 1
    
    ***REMOVED*** è®¡ç®—å¹³å‡å€¼
    if word_diffs:
        stats["word_control"]["avg_diff_percent"] = sum(word_diffs) / len(word_diffs)
    
    if stats["total_chapters"] > 0:
        stats["quality"]["avg_issues_per_chapter"] = stats["quality"]["total_issues"] / stats["total_chapters"]
    
    if stats["rewrite"]["rewritten_chapters"] > 0:
        stats["rewrite"]["avg_rewrite_rounds"] = total_rewrite_rounds / stats["rewrite"]["rewritten_chapters"]
        stats["rewrite"]["improvement_rate"] = stats["rewrite"]["improved_chapters"] / stats["rewrite"]["rewritten_chapters"] * 100
    
    return stats


def compare_results(results: Dict[str, Dict[str, Any]]) -> str:
    """
    å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š
    """
    report = []
    report.append("=" * 80)
    report.append("LLMæ¨¡å‹å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š")
    report.append("=" * 80)
    report.append("")
    
    models = list(results.keys())
    if len(models) != 2:
        return "éœ€è¦ä¸¤ä¸ªæ¨¡å‹çš„æµ‹è¯•ç»“æœè¿›è¡Œå¯¹æ¯”"
    
    model1, model2 = models[0], models[1]
    r1, r2 = results[model1], results[model2]
    
    ***REMOVED*** åŸºæœ¬ä¿¡æ¯
    report.append("æµ‹è¯•é…ç½®:")
    report.append(f"  å°è¯´æ ‡é¢˜: {r1.get('novel_title', 'N/A')}")
    report.append(f"  ç±»å‹: {r1.get('genre', 'N/A')}")
    report.append(f"  ç« èŠ‚æ•°: {r1.get('target_chapters', 'N/A')}")
    report.append("")
    
    ***REMOVED*** å­—æ•°æ§åˆ¶å¯¹æ¯”
    report.append("=" * 80)
    report.append("1. å­—æ•°æ§åˆ¶å¯¹æ¯”")
    report.append("=" * 80)
    
    w1 = r1.get("word_control", {})
    w2 = r2.get("word_control", {})
    
    report.append(f"{model1}:")
    report.append(f"  å¹³å‡å­—æ•°å·®å¼‚: {w1.get('avg_diff_percent', 0):.1f}%")
    report.append(f"  ç›®æ ‡èŒƒå›´å†…ç« èŠ‚: {w1.get('chapters_within_target', 0)}/{r1.get('total_chapters', 0)}")
    report.append(f"  è¶…å‡ºç›®æ ‡ç« èŠ‚: {w1.get('chapters_over_target', 0)}/{r1.get('total_chapters', 0)}")
    report.append("")
    
    report.append(f"{model2}:")
    report.append(f"  å¹³å‡å­—æ•°å·®å¼‚: {w2.get('avg_diff_percent', 0):.1f}%")
    report.append(f"  ç›®æ ‡èŒƒå›´å†…ç« èŠ‚: {w2.get('chapters_within_target', 0)}/{r2.get('total_chapters', 0)}")
    report.append(f"  è¶…å‡ºç›®æ ‡ç« èŠ‚: {w2.get('chapters_over_target', 0)}/{r2.get('total_chapters', 0)}")
    report.append("")
    
    diff1 = abs(w1.get('avg_diff_percent', 0))
    diff2 = abs(w2.get('avg_diff_percent', 0))
    if diff1 < diff2:
        report.append(f"âœ… {model1} å­—æ•°æ§åˆ¶æ›´å‡†ç¡®ï¼ˆå·®å¼‚ {diff1:.1f}% vs {diff2:.1f}%ï¼‰")
    elif diff2 < diff1:
        report.append(f"âœ… {model2} å­—æ•°æ§åˆ¶æ›´å‡†ç¡®ï¼ˆå·®å¼‚ {diff2:.1f}% vs {diff1:.1f}%ï¼‰")
    else:
        report.append(f"âš–ï¸ å­—æ•°æ§åˆ¶ç›¸å½“ï¼ˆå·®å¼‚ {diff1:.1f}% vs {diff2:.1f}%ï¼‰")
    report.append("")
    
    ***REMOVED*** è´¨é‡é—®é¢˜å¯¹æ¯”
    report.append("=" * 80)
    report.append("2. è´¨é‡é—®é¢˜å¯¹æ¯”")
    report.append("=" * 80)
    
    q1 = r1.get("quality", {})
    q2 = r2.get("quality", {})
    
    report.append(f"{model1}:")
    report.append(f"  æ€»é—®é¢˜æ•°: {q1.get('total_issues', 0)}")
    report.append(f"  å¹³å‡æ¯ç« é—®é¢˜æ•°: {q1.get('avg_issues_per_chapter', 0):.2f}")
    report.append(f"  æœ‰é—®é¢˜çš„ç« èŠ‚: {q1.get('chapters_with_issues', 0)}/{r1.get('total_chapters', 0)}")
    report.append(f"  ä¸¥é‡é—®é¢˜æ•°: {q1.get('high_severity_issues', 0)}")
    report.append("")
    
    report.append(f"{model2}:")
    report.append(f"  æ€»é—®é¢˜æ•°: {q2.get('total_issues', 0)}")
    report.append(f"  å¹³å‡æ¯ç« é—®é¢˜æ•°: {q2.get('avg_issues_per_chapter', 0):.2f}")
    report.append(f"  æœ‰é—®é¢˜çš„ç« èŠ‚: {q2.get('chapters_with_issues', 0)}/{r2.get('total_chapters', 0)}")
    report.append(f"  ä¸¥é‡é—®é¢˜æ•°: {q2.get('high_severity_issues', 0)}")
    report.append("")
    
    avg1 = q1.get('avg_issues_per_chapter', 0)
    avg2 = q2.get('avg_issues_per_chapter', 0)
    if avg1 < avg2:
        report.append(f"âœ… {model1} è´¨é‡é—®é¢˜æ›´å°‘ï¼ˆå¹³å‡ {avg1:.2f} vs {avg2:.2f} ä¸ª/ç« ï¼‰")
    elif avg2 < avg1:
        report.append(f"âœ… {model2} è´¨é‡é—®é¢˜æ›´å°‘ï¼ˆå¹³å‡ {avg2:.2f} vs {avg1:.2f} ä¸ª/ç« ï¼‰")
    else:
        report.append(f"âš–ï¸ è´¨é‡é—®é¢˜ç›¸å½“ï¼ˆå¹³å‡ {avg1:.2f} vs {avg2:.2f} ä¸ª/ç« ï¼‰")
    report.append("")
    
    ***REMOVED*** é‡å†™æ•ˆæœå¯¹æ¯”
    report.append("=" * 80)
    report.append("3. é‡å†™æ•ˆæœå¯¹æ¯”")
    report.append("=" * 80)
    
    rw1 = r1.get("rewrite", {})
    rw2 = r2.get("rewrite", {})
    
    report.append(f"{model1}:")
    report.append(f"  é‡å†™ç« èŠ‚æ•°: {rw1.get('rewritten_chapters', 0)}/{r1.get('total_chapters', 0)}")
    report.append(f"  æ”¹å–„ç« èŠ‚: {rw1.get('improved_chapters', 0)} ({rw1.get('improvement_rate', 0):.1f}%)")
    report.append(f"  æœªå˜åŒ–ç« èŠ‚: {rw1.get('unchanged_chapters', 0)}")
    report.append(f"  æ¶åŒ–ç« èŠ‚: {rw1.get('worsened_chapters', 0)}")
    report.append(f"  å¹³å‡é‡å†™è½®æ•°: {rw1.get('avg_rewrite_rounds', 0):.2f}")
    report.append("")
    
    report.append(f"{model2}:")
    report.append(f"  é‡å†™ç« èŠ‚æ•°: {rw2.get('rewritten_chapters', 0)}/{r2.get('total_chapters', 0)}")
    report.append(f"  æ”¹å–„ç« èŠ‚: {rw2.get('improved_chapters', 0)} ({rw2.get('improvement_rate', 0):.1f}%)")
    report.append(f"  æœªå˜åŒ–ç« èŠ‚: {rw2.get('unchanged_chapters', 0)}")
    report.append(f"  æ¶åŒ–ç« èŠ‚: {rw2.get('worsened_chapters', 0)}")
    report.append(f"  å¹³å‡é‡å†™è½®æ•°: {rw2.get('avg_rewrite_rounds', 0):.2f}")
    report.append("")
    
    rate1 = rw1.get('improvement_rate', 0)
    rate2 = rw2.get('improvement_rate', 0)
    if rate1 > rate2:
        report.append(f"âœ… {model1} é‡å†™æ”¹å–„ç‡æ›´é«˜ï¼ˆ{rate1:.1f}% vs {rate2:.1f}%ï¼‰")
    elif rate2 > rate1:
        report.append(f"âœ… {model2} é‡å†™æ”¹å–„ç‡æ›´é«˜ï¼ˆ{rate2:.1f}% vs {rate1:.1f}%ï¼‰")
    else:
        report.append(f"âš–ï¸ é‡å†™æ”¹å–„ç‡ç›¸å½“ï¼ˆ{rate1:.1f}% vs {rate2:.1f}%ï¼‰")
    report.append("")
    
    ***REMOVED*** ç»¼åˆå»ºè®®
    report.append("=" * 80)
    report.append("4. ç»¼åˆå»ºè®®")
    report.append("=" * 80)
    
    recommendations = []
    
    ***REMOVED*** å­—æ•°æ§åˆ¶å»ºè®®
    if diff1 < diff2:
        recommendations.append(f"ğŸ“ ç« èŠ‚ç”Ÿæˆï¼šæ¨èä½¿ç”¨ {model1}ï¼ˆå­—æ•°æ§åˆ¶æ›´å‡†ç¡®ï¼‰")
    elif diff2 < diff1:
        recommendations.append(f"ğŸ“ ç« èŠ‚ç”Ÿæˆï¼šæ¨èä½¿ç”¨ {model2}ï¼ˆå­—æ•°æ§åˆ¶æ›´å‡†ç¡®ï¼‰")
    
    ***REMOVED*** è´¨é‡é—®é¢˜å»ºè®®
    if avg1 < avg2:
        recommendations.append(f"âœ… åˆå§‹è´¨é‡ï¼š{model1} ç”Ÿæˆè´¨é‡æ›´å¥½ï¼ˆé—®é¢˜æ›´å°‘ï¼‰")
    elif avg2 < avg1:
        recommendations.append(f"âœ… åˆå§‹è´¨é‡ï¼š{model2} ç”Ÿæˆè´¨é‡æ›´å¥½ï¼ˆé—®é¢˜æ›´å°‘ï¼‰")
    
    ***REMOVED*** é‡å†™æ•ˆæœå»ºè®®
    if rate1 > rate2:
        recommendations.append(f"ğŸ”„ é‡å†™æœºåˆ¶ï¼šæ¨èä½¿ç”¨ {model1}ï¼ˆæ”¹å–„ç‡æ›´é«˜ï¼‰")
    elif rate2 > rate1:
        recommendations.append(f"ğŸ”„ é‡å†™æœºåˆ¶ï¼šæ¨èä½¿ç”¨ {model2}ï¼ˆæ”¹å–„ç‡æ›´é«˜ï¼‰")
    
    if not recommendations:
        recommendations.append("âš–ï¸ ä¸¤ä¸ªæ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œå¯æ ¹æ®æˆæœ¬ã€é€Ÿåº¦ç­‰å› ç´ é€‰æ‹©")
    
    for rec in recommendations:
        report.append(rec)
    
    report.append("")
    report.append("=" * 80)
    
    return "\n".join(report)


def main():
    parser = argparse.ArgumentParser(description="LLMæ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    parser.add_argument("--novel-title", type=str, default="å¯¹æ¯”æµ‹è¯•å°è¯´", help="å°è¯´æ ‡é¢˜")
    parser.add_argument("--genre", type=str, default="ç§‘å¹»", help="å°è¯´ç±»å‹")
    parser.add_argument("--theme", type=str, default="æ—¶é—´æ—…è¡Œã€å¹³è¡Œä¸–ç•Œã€è®°å¿†ç¢ç‰‡", help="ä¸»é¢˜")
    parser.add_argument("--chapters", type=int, default=10, help="ç« èŠ‚æ•°ï¼ˆå»ºè®®10ç« è¿›è¡Œå¿«é€Ÿå¯¹æ¯”ï¼‰")
    parser.add_argument("--words", type=int, default=2048, help="æ¯ç« ç›®æ ‡å­—æ•°")
    parser.add_argument("--test-title", type=str, default="LLMå¯¹æ¯”æµ‹è¯•", help="æµ‹è¯•æ ‡é¢˜")
    
    args = parser.parse_args()
    
    ***REMOVED*** æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹
    models = ["gemini_3_flash", "deepseek_v3_2"]
    results = {}
    
    for model in models:
        try:
            stats = test_with_model(
                model_name=model,
                novel_title=args.novel_title,
                genre=args.genre,
                theme=args.theme,
                target_chapters=args.chapters,
                words_per_chapter=args.words,
                test_title=args.test_title
            )
            
            ***REMOVED*** æ·»åŠ æµ‹è¯•é…ç½®ä¿¡æ¯
            stats["novel_title"] = args.novel_title
            stats["genre"] = args.genre
            stats["target_chapters"] = args.chapters
            
            results[model] = stats
            
            ***REMOVED*** ä¿å­˜å•ä¸ªæ¨¡å‹çš„ç»“æœ
            result_file = Path(f"novel_creation/outputs/{args.test_title}_{model}/comparison_result.json")
            result_file.parent.mkdir(parents=True, exist_ok=True)
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            logger.info(f"{model} æµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            logger.error(f"{model} æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
            results[model] = {
                "model": model,
                "error": str(e),
                "success": False
            }
    
    ***REMOVED*** ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if len(results) == 2 and all(r.get("success", False) for r in results.values()):
        report = compare_results(results)
        print("\n" + report)
        
        ***REMOVED*** ä¿å­˜å¯¹æ¯”æŠ¥å‘Š
        report_file = Path(f"novel_creation/outputs/{args.test_title}_comparison_report.txt")
        report_file.parent.mkdir(parents=True, exist_ok=True)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    else:
        logger.warning("éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆå®Œæ•´å¯¹æ¯”æŠ¥å‘Š")


if __name__ == "__main__":
    main()
