***REMOVED***!/usr/bin/env python3
"""
åˆ†æã€Šé—å¿˜å›¾ä¹¦é¦†ã€‹æµ‹è¯•ç»“æœ
"""
import json
import os
from pathlib import Path

def analyze_test_results():
    """åˆ†ææµ‹è¯•ç»“æœ"""
    ***REMOVED*** è‡ªåŠ¨æŸ¥æ‰¾metadata.jsonæ–‡ä»¶
    outputs_dir = "outputs"
    metadata_path = None
    
    ***REMOVED*** å…ˆå°è¯•"é—å¿˜å›¾ä¹¦é¦†"ç›®å½•
    if os.path.exists(f"{outputs_dir}/é—å¿˜å›¾ä¹¦é¦†/metadata.json"):
        metadata_path = f"{outputs_dir}/é—å¿˜å›¾ä¹¦é¦†/metadata.json"
    else:
        ***REMOVED*** æŸ¥æ‰¾åŒ…å«"é—å¿˜å›¾ä¹¦é¦†"å®Œæ•´ç‰ˆtxtçš„ç›®å½•
        for dirname in os.listdir(outputs_dir):
            dirpath = os.path.join(outputs_dir, dirname)
            if os.path.isdir(dirpath):
                if os.path.exists(os.path.join(dirpath, "metadata.json")):
                    ***REMOVED*** æ£€æŸ¥æ˜¯å¦åŒ…å«"é—å¿˜å›¾ä¹¦é¦†"
                    txt_files = [f for f in os.listdir(dirpath) if f.endswith("_å®Œæ•´ç‰ˆ.txt")]
                    if any("é—å¿˜å›¾ä¹¦é¦†" in f for f in txt_files):
                        metadata_path = os.path.join(dirpath, "metadata.json")
                        break
                ***REMOVED*** å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æœ€æ–°çš„ç›®å½•
                if metadata_path is None and os.path.exists(os.path.join(dirpath, "metadata.json")):
                    metadata_path = os.path.join(dirpath, "metadata.json")
    
    if not metadata_path or not os.path.exists(metadata_path):
        print("âŒ æµ‹è¯•å°šæœªå®Œæˆï¼Œmetadata.json æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    output_dir = os.path.dirname(metadata_path)
    
    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    print("=" * 80)
    print("ã€Šé—å¿˜å›¾ä¹¦é¦†ã€‹æµ‹è¯•ç»“æœåˆ†æ")
    print("=" * 80)
    print()
    
    ***REMOVED*** åŸºæœ¬ä¿¡æ¯
    print(f"ğŸ“– å°è¯´ä¿¡æ¯:")
    print(f"  æ ‡é¢˜: {metadata['title']}")
    print(f"  ç±»å‹: {metadata['genre']}")
    print(f"  ä¸»é¢˜: {metadata['theme']}")
    print(f"  æ€»ç« èŠ‚: {metadata['total_chapters']}ç« ")
    print(f"  æ€»å­—æ•°: {metadata['total_words']:,}å­—")
    print(f"  å¹³å‡æ¯ç« : {metadata['total_words'] // metadata['total_chapters']}å­—")
    print()
    
    ***REMOVED*** è´¨é‡è¯„åˆ†
    periodic_checks = metadata.get('periodic_quality_checks', [])
    if periodic_checks:
        print(f"ğŸ“ˆ é˜¶æ®µæ€§è´¨é‡è¯„åˆ†:")
        for check in periodic_checks:
            scores = check.get('scores', {})
            print(f"  {check.get('chapter_range', 'æœªçŸ¥')}:")
            print(f"    ç»¼åˆè¯„åˆ†: {scores.get('overall', 0):.2f}")
            print(f"    è¿è´¯æ€§: {scores.get('coherence', 0):.2f}")
            print(f"    äººç‰©: {scores.get('character_consistency', 0):.2f}")
            print(f"    èŠ‚å¥: {scores.get('plot_rhythm', 0):.2f}")
            print(f"    ä¸–ç•Œè§‚: {scores.get('worldview_consistency', 0):.2f}")
            suspense_score = scores.get('suspense', 0)
            if suspense_score >= 0.6:
                print(f"    æ‚¬å¿µ: {suspense_score:.2f} âœ… (ä¼˜ç§€)")
            elif suspense_score >= 0.4:
                print(f"    æ‚¬å¿µ: {suspense_score:.2f} âš ï¸ (è‰¯å¥½)")
            else:
                print(f"    æ‚¬å¿µ: {suspense_score:.2f} âŒ (åä½)")
            print()
    
    ***REMOVED*** è´¨é‡é—®é¢˜ç»Ÿè®¡
    quality_tracker = metadata.get('quality_tracker', {})
    chapter_history = quality_tracker.get('chapter_quality_history', [])
    
    if chapter_history:
        total_issues = sum(c.get('quality_issues', 0) for c in chapter_history)
        avg_issues = total_issues / len(chapter_history) if chapter_history else 0
        
        print(f"ğŸ“Š è´¨é‡é—®é¢˜ç»Ÿè®¡:")
        print(f"  æ€»é—®é¢˜æ•°: {total_issues}ä¸ª")
        print(f"  å¹³å‡æ¯ç« : {avg_issues:.1f}ä¸ªé—®é¢˜")
        print(f"  ä¸¥é‡é—®é¢˜ç« èŠ‚: {metadata.get('quality_check', {}).get('high_severity_chapters', 0)}ç« ")
        print()
        
        ***REMOVED*** ç»Ÿè®¡é‡å†™æƒ…å†µ
        rewritten_chapters = []
        for chapter_info in chapter_history:
            chapter_num = chapter_info['chapter_number']
            meta_file = os.path.join(output_dir, "chapters", f"chapter_{chapter_num:03d}_meta.json")
            if os.path.exists(meta_file):
                with open(meta_file, 'r', encoding='utf-8') as f:
                    chapter_meta = json.load(f)
                    if chapter_meta.get('metadata', {}).get('rewritten', False):
                        original_issues = chapter_info.get('quality_issues', 0)
                        rewritten_issues = chapter_meta.get('metadata', {}).get('quality_check_after_rewrite', {}).get('total_issues', 0)
                        rewritten_chapters.append({
                            'chapter': chapter_num,
                            'original_issues': original_issues,
                            'rewritten_issues': rewritten_issues,
                            'improved': rewritten_issues < original_issues
                        })
        
        if rewritten_chapters:
            print(f"ğŸ”„ é‡å†™æœºåˆ¶ç»Ÿè®¡:")
            print(f"  é‡å†™ç« èŠ‚æ•°: {len(rewritten_chapters)}ç«  ({len(rewritten_chapters)/len(chapter_history)*100:.1f}%)")
            improved = sum(1 for c in rewritten_chapters if c['improved'])
            unchanged = sum(1 for c in rewritten_chapters if not c['improved'] and c['rewritten_issues'] == c['original_issues'])
            worsened = sum(1 for c in rewritten_chapters if c['rewritten_issues'] > c['original_issues'])
            print(f"  æ”¹å–„: {improved}ç«  ({improved/len(rewritten_chapters)*100:.1f}%)")
            print(f"  æ— å˜åŒ–: {unchanged}ç«  ({unchanged/len(rewritten_chapters)*100:.1f}%)")
            print(f"  æ¶åŒ–: {worsened}ç«  ({worsened/len(rewritten_chapters)*100:.1f}%)")
            print()
    
    ***REMOVED*** å­—æ•°æ§åˆ¶
    word_deviations = []
    for c in chapter_history:
        target = c.get('target_words', 2048)
        actual = c.get('word_count', 0)
        if target > 0:
            deviation = (actual - target) / target * 100
            word_deviations.append(deviation)
    
    if word_deviations:
        avg_deviation = sum(word_deviations) / len(word_deviations)
        max_deviation = max(abs(d) for d in word_deviations)
        print(f"ğŸ“ å­—æ•°æ§åˆ¶:")
        print(f"  å¹³å‡åå·®: {avg_deviation:+.1f}%")
        print(f"  æœ€å¤§åå·®: {max_deviation:.1f}%")
        print()
    
    ***REMOVED*** å…³é”®å‘ç°
    print("=" * 80)
    print("å…³é”®å‘ç°:")
    print("=" * 80)
    
    if periodic_checks:
        latest_check = periodic_checks[-1]
        suspense_score = latest_check.get('scores', {}).get('suspense', 0)
        overall_score = latest_check.get('scores', {}).get('overall', 0)
        
        if suspense_score >= 0.6:
            print(f"âœ… æ‚¬å¿µå¾—åˆ†ä¼˜ç§€ ({suspense_score:.2f})ï¼Œæ‚¬ç–‘ä¸»é¢˜ç¡®å®æå‡äº†æ‚¬å¿µè¡¨ç°")
        elif suspense_score >= 0.4:
            print(f"âš ï¸  æ‚¬å¿µå¾—åˆ†è‰¯å¥½ ({suspense_score:.2f})ï¼Œæ¯”æ²»æ„ˆå‘ä¸»é¢˜æœ‰æ‰€æå‡ï¼Œä½†ä»éœ€ä¼˜åŒ–")
        else:
            print(f"âŒ æ‚¬å¿µå¾—åˆ†åä½ ({suspense_score:.2f})ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æ‚¬å¿µæœºåˆ¶")
        
        if overall_score >= 0.85:
            print(f"âœ… ç»¼åˆè¯„åˆ†ä¼˜ç§€ ({overall_score:.2f})ï¼Œè¾¾åˆ°ä¼˜è´¨æ ‡å‡†")
        elif overall_score >= 0.75:
            print(f"âš ï¸  ç»¼åˆè¯„åˆ†è‰¯å¥½ ({overall_score:.2f})ï¼Œæ¥è¿‘ä¼˜è´¨æ ‡å‡†")
        else:
            print(f"âŒ ç»¼åˆè¯„åˆ†åä½ ({overall_score:.2f})ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print("=" * 80)

if __name__ == "__main__":
    os.chdir(Path(__file__).parent)
    analyze_test_results()
